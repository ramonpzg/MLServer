---
title: "Examples"
---

To see MLServer in action you can check out the examples below.
These are end-to-end notebooks, showing how to serve models with MLServer.

## Inference Runtimes

If you are interested in how MLServer interacts with particular model
frameworks, you can check the following examples.
These focus on showcasing the different [inference
runtimes](../runtimes/index.mdx) that ship with MLServer out of the box.
Note that, for **advanced use cases**, you can also write your own custom
inference runtime (see the [example below on custom
models](./custom/README.mdx)).

- [Serving Scikit-Learn models](./sklearn/README.mdx)
- [Serving XGBoost models](./xgboost/README.mdx)
- [Serving LightGBM models](./lightgbm/README.mdx)
- [Serving CatBoost models](./catboost/README.mdx)
- [Serving Tempo pipelines](./tempo/README.mdx)
- [Serving MLflow models](./mlflow/README.mdx)
- [Serving custom models](./custom/README.mdx)
- [Serving Alibi Detect models](./alibi-detect/README.mdx)
- [Serving HuggingFace models](./huggingface/README.mdx)

## MLServer Features

To see some of the advanced features included in MLServer (e.g. multi-model
serving), check out the examples below.

- [Multi-Model Serving with multiple frameworks](./mms/README.mdx)
- [Loading / unloading models from a model repository](./model-repository/README.mdx)
- [Content-Type Decoding](./content-type/README.mdx)
- [Custom Conda environment](./conda/README.mdx)
- [Serving custom models requiring JSON inputs or outputs](./custom-json/README.mdx)
- [Serving models through Kafka](./kafka/README.mdx)


## Tutorials

Tutorials are designed to be *beginner-friendly* and walk through accomplishing a series of tasks using MLServer (and other tools). 

- [Deploying a Custom Tensorflow Model with MLServer and Seldon Core](./cassava/README.md)
